{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d31755-1dc7-43be-913c-272771e25b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suramya/miniconda3/lib/python3.12/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   /home/suramya/miniconda3\n",
      "cc3d                   /home/suramya/miniconda3/envs/cc3d\n",
      "cc3d_4.7.0             /home/suramya/miniconda3/envs/cc3d_4.7.0\n",
      "ebay_ml              * /home/suramya/miniconda3/envs/ebay_ml\n",
      "llm                    /home/suramya/miniconda3/envs/llm\n",
      "llm_2                  /home/suramya/miniconda3/envs/llm_2\n",
      "med_diag_env           /home/suramya/miniconda3/envs/med_diag_env\n",
      "medical_bot            /home/suramya/miniconda3/envs/medical_bot\n",
      "qiskit-env             /home/suramya/miniconda3/envs/qiskit-env\n",
      "tlens                  /home/suramya/miniconda3/envs/tlens\n",
      "torch_env              /home/suramya/miniconda3/envs/torch_env\n",
      "vscode.env             /home/suramya/miniconda3/envs/vscode.env\n",
      "                       /home/suramya/vscode.py/.conda\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b589590-faa2-4333-8ac3-8de43a6b13b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000, 3) Index(['record_id', 'category_id', 'title'], dtype='object')\n",
      "(56812, 5) Index(['record_id', 'category_id', 'title', 'token', 'raw_tag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Listing titles (2 million rows)\n",
    "df_listing = pd.read_csv(\n",
    "    \"Listing_Titles.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    keep_default_na=False,  # don't turn \"\" into NaN\n",
    "    na_values=None,\n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "# Tagged train data (≈ 56k rows)\n",
    "df_train = pd.read_csv(\n",
    "    \"Tagged_Titles_Train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    keep_default_na=False,\n",
    "    na_values=None,\n",
    "    dtype=str\n",
    ")\n",
    "df_listing = df_listing.rename(columns={\n",
    "    \"Record Number\": \"record_id\",\n",
    "    \"Category\": \"category_id\",\n",
    "    \"Title\": \"title\"\n",
    "})\n",
    "\n",
    "df_train = df_train.rename(columns={\n",
    "    \"Record Number\": \"record_id\",\n",
    "    \"Category\": \"category_id\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Token\": \"token\",\n",
    "    \"Tag\": \"raw_tag\"\n",
    "})\n",
    "print(df_listing.shape, df_listing.columns)\n",
    "print(df_train.shape, df_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e342eca-4029-4501-8441-796db9e8f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43550/1208407759.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = df_train.groupby('record_id', sort=True).apply(record_to_bio)\n"
     ]
    }
   ],
   "source": [
    "def record_to_bio(group):\n",
    "    tokens = group['token'].tolist()\n",
    "    raw_tags = group['raw_tag'].tolist()\n",
    "    bios = []\n",
    "\n",
    "    prev_tag_nonempty = None  # last seen non-empty, non-\"O\" tag\n",
    "    prev_raw_tag = None       # raw tag of previous token (can be same string, or \"\")\n",
    "\n",
    "    for i, rt in enumerate(raw_tags):\n",
    "        if rt == \"O\":\n",
    "            bios.append(\"O\")\n",
    "            prev_tag_nonempty = None\n",
    "            prev_raw_tag = \"O\"\n",
    "        elif rt == \"\":\n",
    "            # continuation of previous non-empty tag\n",
    "            if prev_tag_nonempty is None:\n",
    "                # edge case: blank but we have nothing to continue -> call it \"O\"\n",
    "                bios.append(\"O\")\n",
    "            else:\n",
    "                bios.append(\"I-\" + prev_tag_nonempty)\n",
    "            prev_raw_tag = \"\"\n",
    "        else:\n",
    "            # new explicit tag\n",
    "            # decide if B- or new B- (always B, because spec says even same tag twice can be two entities)\n",
    "            bios.append(\"B-\" + rt)\n",
    "            if rt != \"O\":\n",
    "                prev_tag_nonempty = rt\n",
    "            else:\n",
    "                prev_tag_nonempty = None\n",
    "            prev_raw_tag = rt\n",
    "\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"bio_tags\": bios,\n",
    "        \"category_id\": group['category_id'].iloc[0],\n",
    "        \"record_id\": group['record_id'].iloc[0],\n",
    "        \"title\": group['title'].iloc[0],\n",
    "    }\n",
    "\n",
    "grouped = df_train.groupby('record_id', sort=True).apply(record_to_bio)\n",
    "train_sequences = list(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c3465a-4cf7-416c-a7e1-dd9c52c840eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = sorted({tag for seq in train_sequences for tag in seq[\"bio_tags\"]})\n",
    "# Example: [\"B-Hersteller\",\"B-Kompatibles_Fahrzeug_Modell\",...,\"I-...\",\"O\"]\n",
    "\n",
    "label2id = {lbl:i for i,lbl in enumerate(all_labels)}\n",
    "id2label = {i:lbl for lbl,i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ae4e81-e8f6-4e06-91bd-8f0b83273954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suramya/miniconda3/envs/ebay_ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_sequence(tokens, bio_tags, category_id):\n",
    "    # prepend synthetic category token\n",
    "    cat_token = f\"[CAT_{category_id}]\"\n",
    "    input_tokens = [cat_token] + tokens\n",
    "    input_tags   = [\"O\"] + bio_tags  # CAT token gets \"O\"\n",
    "\n",
    "    enc = tokenizer(\n",
    "        input_tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=False,\n",
    "        truncation=True,\n",
    "        max_length=128,   # adjust if titles can be longer; most won't\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # enc.word_ids() gives index of which original input_tokens word each subword came from\n",
    "    word_ids = enc.word_ids()\n",
    "\n",
    "    labels = []\n",
    "    for wi in word_ids:\n",
    "        if wi is None:\n",
    "            labels.append(-100)  # special tokens like <s>, </s>\n",
    "        else:\n",
    "            # only label the first subword of each original token\n",
    "            # rule: if this word_id is same as previous word_id, use -100\n",
    "            if len(labels)>0 and word_ids[labels.index(labels[-1]) if labels[-1]!=-100 else len(labels)-1] == wi:\n",
    "                # Actually easier: track prev_wi\n",
    "                pass\n",
    "\n",
    "def encode_sequence(tokens, bio_tags, category_id):\n",
    "    cat_token = f\"[CAT_{category_id}]\"\n",
    "    input_tokens = [cat_token] + tokens\n",
    "    input_tags   = [\"O\"] + bio_tags\n",
    "\n",
    "    enc = tokenizer(\n",
    "        input_tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=False,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    word_ids = enc.word_ids()\n",
    "\n",
    "    labels = []\n",
    "    prev_wi = None\n",
    "    for wi in word_ids:\n",
    "        if wi is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            if wi != prev_wi:\n",
    "                # first subword of this original token → real label id\n",
    "                lbl = input_tags[wi]\n",
    "                labels.append(label2id[lbl])\n",
    "                prev_wi = wi\n",
    "            else:\n",
    "                # continuation subword → ignore in loss\n",
    "                labels.append(-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13aed4c9-4574-4c0b-bfdb-92894e8c31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = [encode_sequence(seq[\"tokens\"], seq[\"bio_tags\"], seq[\"category_id\"])\n",
    "                 for seq in train_sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbceac5-544a-441b-8207-1021ad2301d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, enc_list):\n",
    "        self.data = enc_list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = NERDataset(encoded_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fd2e9f-22be-401b-9f7f-962957b9f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67cae4ec-2a0b-431b-99c9-d4550c2594da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef8df3-ae91-40cb-9571-d8dec0378ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43550/1744690733.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/suramya/miniconda3/envs/ebay_ml/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/939 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ebay_ner_model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    # evaluation_strategy=\"no\",  # baseline: no val split yet\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
